<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="None">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        PySpark学习之路 - undefined
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="../../../../css/aircloud.css">
    <link rel="stylesheet" href="../../../../css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i>  </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar">
            <img src="/" />
        </div>
        <div class="name">
            <i>lamia</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="../../../../index.html">
                    <i class="iconfont icon-shouye1"></i>
                    <span>HOME</span>
                </a>
            </li>
            <li >
                <a href="../../../../tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>TAGS</span>
                </a>
            </li>
            <li >
                <a href="../../../../archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>ARCHIVES</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>ABOUT</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0-安装"><span class="toc-text">0. 安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-安装jdk8"><span class="toc-text">0. 安装jdk8</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-安装Scala"><span class="toc-text">1. 安装Scala</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-安装spark"><span class="toc-text">2. 安装spark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-安装hadoop"><span class="toc-text">3. 安装hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-0-配置hostname"><span class="toc-text">3.0 配置hostname</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-配置ssh无密登陆"><span class="toc-text">3.1 配置ssh无密登陆</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-配置hadoop与yarn"><span class="toc-text">3.2 配置hadoop与yarn</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-0-slaves"><span class="toc-text">3.2.0 slaves</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-core-site-xml"><span class="toc-text">3.2.1 core_site.xml</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-hdfs-site-xml"><span class="toc-text">3.2.2 hdfs-site.xml</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-mapred-site-xml"><span class="toc-text">3.2.3 mapred-site.xml</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-4-yarn-site-xml"><span class="toc-text">3.2.4 yarn-site.xml</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-5-配置环境文件"><span class="toc-text">3.2.5 配置环境文件</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-测试hadoop，yarn，spark"><span class="toc-text">4 测试hadoop，yarn，spark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-0-分发文件"><span class="toc-text">4.0 分发文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-测试"><span class="toc-text">4.1 测试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-0-测试hdfs"><span class="toc-text">4.1.0 测试hdfs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-测试yarn"><span class="toc-text">4.1.1 测试yarn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2-测试spark"><span class="toc-text">4.1.2 测试spark</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Spark简介"><span class="toc-text">1. Spark简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-Spark是什么"><span class="toc-text">0. Spark是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-为什么要有这个Spark"><span class="toc-text">1. 为什么要有这个Spark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-有了Spark能做什么-不能做什么"><span class="toc-text">2. 有了Spark能做什么/不能做什么</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-快速上手"><span class="toc-text">2. 快速上手</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-GetStarted"><span class="toc-text">0. GetStarted</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-利用spark集群跑排序算法"><span class="toc-text">1. 利用spark集群跑排序算法</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">search</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i>  </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        PySpark学习之路
    </div>

    <div class="post-meta">
        <span class="attr">Post：<span>2018-09-14 09:20:15</span></span>
        
        <span class="attr">Tags：/
        
        <a class="tag" href="/tags/#spark, python, hadoop, yarn, 大数据" title="spark, python, hadoop, yarn, 大数据">spark, python, hadoop, yarn, 大数据</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">Visit：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <p>[TOC]</p>
<h1 id="0-安装"><a href="#0-安装" class="headerlink" title="0. 安装"></a>0. 安装</h1><p><strong>注：</strong> 下列安装教程是在Linux环境，主从服务器的登陆账户均为e0024， 所有文件都在/data/e0024/workspace/spark目录下进行，若是Windows系统，export命令等价于添加环境变量</p>
<h2 id="0-安装jdk8"><a href="#0-安装jdk8" class="headerlink" title="0. 安装jdk8"></a>0. 安装jdk8</h2><p>下载<a href="http:/www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">jdk8</a> 并解压到 <strong>/data/e0024/workspace/spark/jdk1.8.0_171</strong>，并在/home/e0024/.bashrc末尾中写入下列语句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/data/e0024/workspace/spark/jdk1.8.0_171</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JRE_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure></p>
<p>然后通过执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /home/e0024/.bashrc</span><br></pre></td></tr></table></figure></p>
<p>刷新环境变量，执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">which</span> java</span><br></pre></td></tr></table></figure></p>
<p>可以验证是否安装成功</p>
<h2 id="1-安装Scala"><a href="#1-安装Scala" class="headerlink" title="1. 安装Scala"></a>1. 安装Scala</h2><ul>
<li>对该语言不熟悉，因此没有安装，若需要用该语言，可按下步骤<br>下载<a href="https:/www.scala-lang.org/download/" target="_blank" rel="noopener">Scala</a>并解压为 <strong>/data/e0024/workspace/spark/scala</strong>，在/home/e0024/.bashrc末尾中写入下列语句<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SCALA_HOME=/data/e0024/workspace/spark/scala</span><br><span class="line">export PATH=$&#123;SCALA_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>刷新环境变量并通过<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">which</span> scala</span><br></pre></td></tr></table></figure></p>
<p>验证是否安装成功</p>
<h2 id="2-安装spark"><a href="#2-安装spark" class="headerlink" title="2. 安装spark"></a>2. 安装spark</h2><p>选择进spark的<a href="http:/spark.apache.org/downloads.html" target="_blank" rel="noopener">官网</a>下载已经编译好的文件<br><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/下载spark.png" alt="enter description here">，解压到<br><strong>/data/e0024/workspace/spark/pyspark-2.7.3</strong>进入该目录执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/data/e0024/workspace/spark/pyspark-2.7.3</span><br><span class="line"><span class="built_in">export</span> PATH=$(SPARK_HOME)/bin:<span class="variable">$&#123;SPARK_HOME&#125;</span>/sbin:<span class="variable">$PATH</span></span><br><span class="line">pip install .   <span class="comment"># 安装pyspark</span></span><br></pre></td></tr></table></figure>
<p>将安装后的pypark安装到spark目录（如果是Aanconda那么目录为：）<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> cp -r <span class="variable">$&#123;HOME&#125;</span>/anaconda3/lib/python3.6/site-packages/pyspark/python <span class="variable">$&#123;SPARK_HOME&#125;</span>/python</span><br><span class="line">pyspark <span class="comment"># 执行pyspark查看安装结果</span></span><br></pre></td></tr></table></figure></p>
<p>若出现<br><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/测试安装spark.png" alt="enter description here"><br>即成功安装</p>
<h2 id="3-安装hadoop"><a href="#3-安装hadoop" class="headerlink" title="3. 安装hadoop"></a>3. 安装hadoop</h2><p>下载<a href="http:/hadoop.apache.org/releases.html" target="_blank" rel="noopener">hadoop</a>，并解压为 <strong>/data/e0024/workspace/spark/hadoop-2.7.6</strong>， 在/home/e0024/.bashrc末尾中写入下列语句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/data/e0024/workspace/spark/hadoop-2.7.6</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export YARN_CONF_DIR=$&#123;YARN_HOME&#125;/etc/hadoop</span><br><span class="line">export PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH</span><br></pre></td></tr></table></figure></p>
<p>刷新环境变量</p>
<h3 id="3-0-配置hostname"><a href="#3-0-配置hostname" class="headerlink" title="3.0 配置hostname"></a>3.0 配置hostname</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/hosts</span><br></pre></td></tr></table></figure>
<p>例如我master0主节点ip为192.168.11.5， slave0从节点ip为192.168.11.6，若有多个slave， 再换行输入<strong>192.168.11.7 slave1</strong>，名字可自己填，不一定要master0， slave0， slave1，即<br>在/etc/hosts文件 内加入如下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">192.168.11.5 master0</span><br><span class="line">192.168.11.6 slave0</span><br></pre></td></tr></table></figure>
<h3 id="3-1-配置ssh无密登陆"><a href="#3-1-配置ssh无密登陆" class="headerlink" title="3.1 配置ssh无密登陆"></a>3.1 配置ssh无密登陆</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -q -t rsa <span class="comment"># 然后一路回车即可， 这个操作需要在每一个主/从节点（master0， slave0等）上操作</span></span><br></pre></td></tr></table></figure>
<p>然后将所有从节点生成的id_rsa.pub汇总到到主节点的 <strong>~/.ssh</strong> 目录下， 注意不要重名覆盖，在master0节点服务器参考如下命令拷贝slave0中的id_rsa.pub， 同理于其他的节点<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp e0024@slave0:/home/e0024/.ssh/id_rsa.pub /home/e0024/.ssh/id_rsa.pub.slave0</span><br></pre></td></tr></table></figure></p>
<p>将所有节点的id_rsa.pub汇集于master0节点后，在master0节点服务器执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/e0024/.ssh</span><br><span class="line">cat id_rsa.pub* &gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure></p>
<p>然后通过 <strong>scp</strong>命令将authorized_keys文件从master0节点服务器分发到所有节点的 <strong>/home/e0024/.ssh</strong>目录下， 分发结束后<br>在master0节点服务器执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh slave0</span><br></pre></td></tr></table></figure>
<p>若不需要密码，直接登录则表示配置成功</p>
<h3 id="3-2-配置hadoop与yarn"><a href="#3-2-配置hadoop与yarn" class="headerlink" title="3.2 配置hadoop与yarn"></a>3.2 配置hadoop与yarn</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop  <span class="comment"># 进入hadoop配置文件目录</span></span><br></pre></td></tr></table></figure>
<h4 id="3-2-0-slaves"><a href="#3-2-0-slaves" class="headerlink" title="3.2.0 slaves"></a>3.2.0 slaves</h4><p>本教程中将master0与slave0都作为工作节点，因此执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> master0 &gt; <span class="variable">$&#123;HADOOP_CONF_DIR&#125;</span>/slaves</span><br><span class="line"><span class="built_in">echo</span> slave0 &gt;&gt; <span class="variable">$&#123;HADOOP_CONF_DIR&#125;</span>/slaves</span><br><span class="line"><span class="built_in">echo</span> master0 &gt; <span class="variable">$&#123;SPARK_HOME&#125;</span>/conf/slaves</span><br><span class="line"><span class="built_in">echo</span> slave0 &gt;&gt; <span class="variable">$&#123;SPARK_HOME&#125;</span>/conf/slaves</span><br></pre></td></tr></table></figure></p>
<p>使用vim编辑这两个slaves文件也是可以的，只要将工作节点的hostname或者ip地址写入进去就可以，每一个节点一行vim， 如下图<br><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/hadoop_slaves.png" alt="enter description here"><br><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/spark-slaves.png" alt="enter description here"></p>
<h4 id="3-2-1-core-site-xml"><a href="#3-2-1-core-site-xml" class="headerlink" title="3.2.1 core_site.xml"></a>3.2.1 core_site.xml</h4><p><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/hadoop_core-site-xml.png" alt="enter description here"><br>图中第一处红线为为开启的hdfs文件系统，写入对应的hdfs:/host:port，端口随意填写，但是不要与别的冲突<br>第二处红线为本地缓存，可以是${HADOOP_HOME}/tmp<br>第三处红线为网页查看dfs状态的端口号，随意填写但是不要与别的端口冲突</p>
<h4 id="3-2-2-hdfs-site-xml"><a href="#3-2-2-hdfs-site-xml" class="headerlink" title="3.2.2 hdfs-site.xml"></a>3.2.2 hdfs-site.xml</h4><p><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/hadoop_hdfs-site-xml.png" alt="enter description here"><br>同理端口号随意设置， 其中dsf.replication 的值不要超过工作节点的个数，如教程中工作节点为master0与slave0共两个，则值设置为1不超过2</p>
<h4 id="3-2-3-mapred-site-xml"><a href="#3-2-3-mapred-site-xml" class="headerlink" title="3.2.3 mapred-site.xml"></a>3.2.3 mapred-site.xml</h4><p><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/hadoop_mapred-site-xml.png" alt="enter description here"></p>
<h4 id="3-2-4-yarn-site-xml"><a href="#3-2-4-yarn-site-xml" class="headerlink" title="3.2.4 yarn-site.xml"></a>3.2.4 yarn-site.xml</h4><p><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/hadoop_yarn-site-xml.png" alt="enter description here"><br>同样，端口号可随意设置， 但不要重复且不与其他服务发生冲突</p>
<h4 id="3-2-5-配置环境文件"><a href="#3-2-5-配置环境文件" class="headerlink" title="3.2.5 配置环境文件"></a>3.2.5 配置环境文件</h4><ol start="0">
<li>打开${SPARK_HOME}/conf/spark_env.sh<br>在文件最后一行加入下述内容，其中 <strong>SPARK_LOCAL_IP</strong>为每个节点服务器的ip地址<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_LOCAL_DIRS=$&#123;SPARK_HOME&#125;</span><br><span class="line">export SPARK_MASTER_IP=192.168.11.5</span><br><span class="line">export SPARK_MASTER_HOST=master0</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line">export SPARK_LOCAL_IP=192.168.11.5</span><br><span class="line">export PYSPARK_PYTHON=$&#123;ANACONDA_HOME&#125;/bin/python</span><br><span class="line">export SPARK_LIBARY_PATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JAVA_HOME&#125;/jre/lib:$&#123;HADOOP_HOME&#125;/lib/native</span><br><span class="line">export LD_LIBRARY_PATH=$&#123;SPARK_LIBARY_PATH&#125;:$&#123;LD_LIBRARY_PATH&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;etc/hadoop</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/spark-env-sh.png" alt="enter description here"></p>
<ol>
<li>打开${HADOOP_CONF_DIR}/hadoop_env.sh<br>并修改JAVA_HOME的值<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=&quot;/data/e0024/workspace/spark/jdk1.8.0_171&quot;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>且在文件最后一行加入下述内容以提供日志记录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_LOG_DIR=&quot;/data/e0024/workspace/spark/hadoop-2.7.6/logs&quot;</span><br></pre></td></tr></table></figure></p>
<ol start="2">
<li>打开${HADOOP_CONF_DIR}/yarn_env.sh<br>修改的内容与方式与hadoop_env.sh一致<h1 id="4-测试hadoop，yarn，spark"><a href="#4-测试hadoop，yarn，spark" class="headerlink" title="4 测试hadoop，yarn，spark"></a>4 测试hadoop，yarn，spark</h1><h2 id="4-0-分发文件"><a href="#4-0-分发文件" class="headerlink" title="4.0 分发文件"></a>4.0 分发文件</h2>教程中所有文件都是在 <strong>/data/e0024/workspace/spark</strong>文件夹内的，且本节操作都是在master0节点服务器上操作，利用 <strong>scp</strong>命令将配置好的jdk，hadoop，spark拷贝到其他服务器中相同的目录下，并且不要忘记修改每台节点服务器的${SPARK_HOME}/conf/spark_env.sh文件中的SPARK_LOCAL_IP， 即<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /data/e0024/workspace/spark e0024@slave0:data/e0024/workspace/spark</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="4-1-测试"><a href="#4-1-测试" class="headerlink" title="4.1 测试"></a>4.1 测试</h2><h3 id="4-1-0-测试hdfs"><a href="#4-1-0-测试hdfs" class="headerlink" title="4.1.0 测试hdfs"></a>4.1.0 测试hdfs</h3><p>执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format <span class="comment"># 格式化，每次修改配置文件后都要执行这语句</span></span><br><span class="line">start-dfs.sh &amp;&amp; jps</span><br></pre></td></tr></table></figure></p>
<p>应该出现下图<br><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/start-dfs-sh.png" alt="enter description here"></p>
<h3 id="4-1-1-测试yarn"><a href="#4-1-1-测试yarn" class="headerlink" title="4.1.1 测试yarn"></a>4.1.1 测试yarn</h3><p>继续执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh &amp;&amp; jps</span><br></pre></td></tr></table></figure></p>
<p>应该出现下图，<br><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/start-yarn-sh.png" alt="enter description here"><br>有ResourceManager出现才证明yarn开启成功，若不成功则查看<br><strong>/data/e0024/workspace/spark/hadoop-2.7.6/logs/yarn-e0024-resourcemanager-IMGPROC-1.log</strong>日志文件，根据错误提示修改前面的配置文件， 若成功继续执行</p>
<h3 id="4-1-2-测试spark"><a href="#4-1-2-测试spark" class="headerlink" title="4.1.2 测试spark"></a>4.1.2 测试spark</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh &amp;&amp; jps</span><br></pre></td></tr></table></figure>
<p>应该出现，其中Worker是因为我在slaves文件中写入了master0所以才会有，如果没有将主节点设置为工作节点Worker是不会有的<br><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/start-all-sh.png" alt="enter description here"></p>
<h1 id="1-Spark简介"><a href="#1-Spark简介" class="headerlink" title="1. Spark简介"></a>1. Spark简介</h1><h2 id="0-Spark是什么"><a href="#0-Spark是什么" class="headerlink" title="0. Spark是什么"></a>0. Spark是什么</h2><h2 id="1-为什么要有这个Spark"><a href="#1-为什么要有这个Spark" class="headerlink" title="1. 为什么要有这个Spark"></a>1. 为什么要有这个Spark</h2><h2 id="2-有了Spark能做什么-不能做什么"><a href="#2-有了Spark能做什么-不能做什么" class="headerlink" title="2. 有了Spark能做什么/不能做什么"></a>2. 有了Spark能做什么/不能做什么</h2><h1 id="2-快速上手"><a href="#2-快速上手" class="headerlink" title="2. 快速上手"></a>2. 快速上手</h1><h2 id="0-GetStarted"><a href="#0-GetStarted" class="headerlink" title="0. GetStarted"></a>0. GetStarted</h2><p>新建测试文件 ‘file.txt’， 内容为<br><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Hello World!!!</span><br><span class="line">Hello laMia~~~</span><br><span class="line">How are you???</span><br><span class="line">Fine, thanks and you???</span><br></pre></td></tr></table></figure></p>
<p>并在同级目录下新建文件 ‘main.py’，统计‘file.txt’中含有‘Hello’的语句有几行，代码如下，python代码中的logger.info, logger.debug可以改成print， 或者logging.info, logging.debug<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> &gt; File: main.py</span></span><br><span class="line"><span class="string"> &gt; Date: 2018/06/11</span></span><br><span class="line"><span class="string"> &gt; Rule: get started with spark in python</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession, SQLContext</span><br><span class="line"><span class="keyword">from</span> common.logger <span class="keyword">import</span> logger</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder\</span><br><span class="line">        .enableHiveSupport()\</span><br><span class="line">        .getOrCreate()</span><br><span class="line"></span><br><span class="line">__LOCALPATH__ = <span class="string">'/home/e0024/workspace/python/spark'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">local_path</span><span class="params">(path)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  p = <span class="string">'file:'</span> + os.path.join(__LOCALPATH__, path)</span><br><span class="line">  logger.debug(<span class="string">'transform file: &#123;&#125; to local path: &#123;&#125;'</span>.format(path, p))</span><br><span class="line">  <span class="keyword">return</span> p</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hdfs_path</span><span class="params">(path)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  p = os.path.join(</span><br><span class="line">        <span class="string">'hdfs:/master0:9000/user/e0024/'</span>, </span><br><span class="line">        path)</span><br><span class="line">  logger.debug(<span class="string">'transform file: &#123;&#125; to hdfs path: &#123;&#125;'</span>.format(path, p))</span><br><span class="line">  <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_spark</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    spark</span><br><span class="line">  <span class="keyword">except</span> NameError <span class="keyword">as</span> ex:</span><br><span class="line">    spark = SparkSession.builder\</span><br><span class="line">            .enableHiveSupport()\</span><br><span class="line">            .getOrCreate()</span><br><span class="line">  logger.info(<span class="string">'Initialize spark'</span>)</span><br><span class="line">  <span class="keyword">return</span> spark</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close_spark</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  spark.stop()</span><br><span class="line">  logger.info(<span class="string">'Close spark'</span>)</span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keep</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    logger.info(<span class="string">'detect alive...'</span>)</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">go_0</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  sc = spark.sparkContext</span><br><span class="line">  sc.setLogLevel(<span class="string">'WARN'</span>)</span><br><span class="line">  textFile = sc.textFile(hdfs_path(<span class="string">'file.txt'</span>))</span><br><span class="line">  logger.info(<span class="string">'rows: &#123;&#125;'</span>.format(textFile.count())) <span class="comment"># to get the number of rows in this DataFrame</span></span><br><span class="line">  logger.info(<span class="string">'first: &#123;&#125;'</span>.format(textFile.first()))</span><br><span class="line">  linesWithCtx = textFile.filter(<span class="keyword">lambda</span> line: <span class="string">'Hello'</span> <span class="keyword">in</span> line)</span><br><span class="line">  logger.info(<span class="string">'ctx:: &#123;&#125;'</span>.format(linesWithCtx.count()))</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">  <span class="string">'''main entry</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  spark = init_spark()</span><br><span class="line">  go_0()</span><br><span class="line">  <span class="comment"># keep()</span></span><br><span class="line">  close_spark()</span><br></pre></td></tr></table></figure></p>
<p>在同级目录下新建调用该main.py的submit.sh脚本，代码如下，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> logger &#123; <span class="comment"># function to print logs</span></span><br><span class="line">  min_lth=40</span><br><span class="line">  src_lth=<span class="variable">$&#123;#1&#125;</span></span><br><span class="line">  actual_lth=<span class="variable">$&#123;min_lth&#125;</span></span><br><span class="line">  blank_lth=1</span><br><span class="line">  sub_lth=$(expr <span class="variable">$&#123;src_lth&#125;</span> - <span class="variable">$&#123;min_lth&#125;</span>)</span><br><span class="line">  TOP=<span class="string">""</span></span><br><span class="line">  content=<span class="string">"<span class="variable">$&#123;1&#125;</span>"</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$&#123;sub_lth&#125;</span> -ge $(expr 2 + 2 \* <span class="variable">$&#123;blank_lth&#125;</span>) ]; \</span><br><span class="line">  <span class="keyword">then</span> \</span><br><span class="line">    actual_lth=$(expr <span class="variable">$&#123;src_lth&#125;</span> + 2 + 2 \* <span class="variable">$&#123;blank_lth&#125;</span>); \</span><br><span class="line">  <span class="keyword">else</span> \</span><br><span class="line">    blank_lth=$(expr <span class="variable">$&#123;min_lth&#125;</span> - <span class="variable">$&#123;src_lth&#125;</span> - 2); \</span><br><span class="line">    blank_lth=$(expr <span class="variable">$&#123;blank_lth&#125;</span> / 2); \</span><br><span class="line">  <span class="keyword">fi</span>; \</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># splice bound line</span></span><br><span class="line">  <span class="keyword">for</span>((i=1;i&lt;=<span class="variable">$&#123;actual_lth&#125;</span>;i++)); \</span><br><span class="line">  <span class="keyword">do</span> \</span><br><span class="line">    TOP=<span class="string">"<span class="variable">$&#123;TOP&#125;</span>*"</span>; \</span><br><span class="line">  <span class="keyword">done</span>; \</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># splice content</span></span><br><span class="line">  <span class="keyword">for</span>((i=1;i&lt;=<span class="variable">$&#123;blank_lth&#125;</span>;i++)); \</span><br><span class="line">  <span class="keyword">do</span> \</span><br><span class="line">    content=<span class="string">" <span class="variable">$&#123;content&#125;</span> "</span>; \</span><br><span class="line">  <span class="keyword">done</span>; \</span><br><span class="line">  content=<span class="string">"*<span class="variable">$&#123;content&#125;</span>*"</span></span><br><span class="line">  </span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;TOP&#125;</span>"</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;content&#125;</span>"</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;TOP&#125;</span>"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -e</span><br><span class="line">clear</span><br><span class="line">logger <span class="string">"START"</span></span><br><span class="line"></span><br><span class="line">spark-submit \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--name sparker_test \</span><br><span class="line">main.py \</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">logger <span class="string">"SUCCEED"</span></span><br></pre></td></tr></table></figure></p>
<p>清理缓存并打开hadoop, yarn, spark服务并将所需的文件file.txt上传到hdfs文件系统中<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">logger <span class="string">"Start to acvitate spark"</span></span><br><span class="line">rm -rf <span class="variable">$&#123;HADOOP_HOME&#125;</span>/tmp</span><br><span class="line">rm -rf <span class="variable">$&#123;HADOOP_HOME&#125;</span>/dfs/data/*</span><br><span class="line">rm -rf <span class="variable">$&#123;HADOOP_HOME&#125;</span>/dfs/name/*</span><br><span class="line">rm -rf <span class="variable">$&#123;HADOOP_HOME&#125;</span>/logs/*</span><br><span class="line">hadoop namenode -format</span><br><span class="line">start-dfs.sh &amp;&amp; start-yarn.sh &amp;&amp; start-all.sh</span><br><span class="line">logger <span class="string">"&gt;&gt; spark in <span class="variable">$&#123;SPARK_HOME&#125;</span> have been acvitated!"</span></span><br><span class="line">hdfs dfs -mkdir -p hdfs:/master0:9000/user/e0024</span><br><span class="line">hdfs dfs -put file.txt hdfs:/master0:9000/user/e0024/file.txt</span><br></pre></td></tr></table></figure></p>
<p>将任务上传并执行，应该会出现以下结果<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x ./submit.sh &amp;&amp; ./submit.sh</span><br></pre></td></tr></table></figure></p>
<p><img src="/2018/09/14/PySpark学习之路/images/PySpark学习之路/getstarted-0.png" alt="enter description here"></p>
<h2 id="1-利用spark集群跑排序算法"><a href="#1-利用spark集群跑排序算法" class="headerlink" title="1. 利用spark集群跑排序算法"></a>1. 利用spark集群跑排序算法</h2>
        
        <div id="comment-container">
        </div>
    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = ""
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="../../../../js/index.js"></script>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

</html>
